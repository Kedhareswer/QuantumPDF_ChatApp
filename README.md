# QuantumPDF ChatApp

**QuantumPDF ChatApp** is an intelligent web application designed to revolutionize how you interact with your PDF documents. By leveraging the power of advanced Large Language Models (LLMs), this application allows you to "chat" with your PDFs, asking questions and receiving insightful answers based on their content. Whether you're dealing with dense research papers, lengthy reports, or technical manuals, QuantumPDF ChatApp transforms your static documents into dynamic conversational partners.

The application seamlessly integrates both locally hosted Hugging Face models and powerful cloud-based LLM APIs, offering flexibility and scalability. It features robust text extraction, intelligent chunking, efficient vector search, and a user-friendly interface to manage and query your documents.

If you find QuantumPDF ChatApp helpful or interesting, please consider [giving it a star on GitHub](https://github.com/Kedhareswer/QuantumPDF_ChatApp)! Your support helps the project grow and reach more users.

---

## Features

QuantumPDF ChatApp is packed with features to make your document interactions seamless and insightful:

-   **Intelligent PDF Processing:**
    -   **Advanced Text Extraction:** Utilizes PyMuPDF (Fitz) to accurately extract text content from your PDF documents, including layout preservation where possible.
    -   **Smart Text Chunking:** Divides extracted text into optimized, overlapping chunks perfect for LLM context windows and effective embedding.
-   **Powerful Search & Retrieval:**
    -   **Advanced Semantic Search:** Employs FAISS vector similarity search on document embeddings (generated by Sentence Transformers) to quickly find the most relevant information for your queries.
    -   **Multi-Document Querying:** Upload and chat with multiple PDF documents within a single session, getting consolidated answers from your entire collection.
-   **Flexible LLM Integration:**
    -   **Broad Model Support:** Connect to a wide array of Large Language Models.
        -   **Local Models:** Natively supports popular Hugging Face transformer models (e.g., `microsoft/DialoGPT-medium`, `HuggingFaceH4/zephyr-7b-beta`, `mistralai/Mistral-7B-Instruct-v0.1`). Run LLMs on your own hardware for privacy and control.
        -   **Cloud APIs:** Easily integrate with leading LLM providers including OpenAI (e.g., GPT-3.5-turbo), Google Gemini (e.g., Gemini-Pro), and AIML API (providing access to models like GPT-4o-mini, Claude 3 Sonnet, Llama 3.1, and more).
-   **Engaging User Experience:**
    -   **Interactive Chat Interface:** A real-time, responsive web interface (built with Next.js and React) for natural conversation with your documents.
    -   **Source Citations:** Clearly see which parts of which documents were used to generate the answer, ensuring transparency and trust.
    -   **Document Management:** Upload, view, and remove documents within your current session.
    -   **Conversation History:** Remembers your ongoing conversation, allowing the LLM to use previous turns as context for more relevant responses.
    -   **Dark & Light Modes:** Choose your preferred visual theme.
-   **Robust & Developer-Friendly:**
    -   **Persistent Document Storage:** Leverages SQLite to save document metadata and text chunks, allowing sessions and processed data to persist.
    -   **Session Management:** Organizes user interactions and documents into distinct sessions.
    -   **Configurable & Extensible:**
        -   Easily add new LLM models (local or cloud) with code modifications in the backend.
        -   Configure API keys and other settings through environment variables (`.env` file).
        -   Default embedding model (`sentence-transformers/all-MiniLM-L6-v2`) can be changed.
        -   While FAISS is the default, the architecture allows for integration of other vector databases.
    -   **Health Check Endpoint:** Provides a `/health` API endpoint for monitoring application status.
    -   **Responsive Design:** Optimized for use on desktops and tablets.
-   **Experimentation & Observability (Optional):**
    -   **Weights & Biases Integration:** Track your experiments, model performance, and data lineage if you choose to enable W&B.
    -   **Comprehensive Logging:** Detailed logging for easier debugging and monitoring.

---

## How it Works

QuantumPDF ChatApp employs a Retrieval Augmented Generation (RAG) architecture to allow you to intelligently query your PDF documents. Here's a step-by-step breakdown of the process:

1.  **User Interaction & Session Start:**
    *   You access the application through a web interface (built with Next.js).
    *   A unique session is initiated to manage your documents and conversation.

2.  **Document Upload & Initial Processing:**
    *   You upload one or more PDF files through the interface.
    *   The Flask backend API receives the files.
    *   **Text Extraction:** PyMuPDF (Fitz) library meticulously extracts text content from each PDF. Metadata like page count and title is also captured.
    *   **Text Chunking:** The extracted text is segmented into smaller, manageable, and slightly overlapping chunks. This is crucial for fitting content into the context window of LLMs and for effective embedding.
    *   **Persistent Storage:** The document metadata and its text chunks are stored in a local SQLite database (`documents.db`), associated with your session. This allows documents to be reused across interactions without reprocessing.

3.  **Embedding & Indexing:**
    *   **Embedding Generation:** Each text chunk is converted into a numerical vector representation (embedding) using a Sentence Transformer model (e.g., `all-MiniLM-L6-v2`). These embeddings capture the semantic meaning of the text.
    *   **Vector Indexing (FAISS):**
        *   For each document, its chunk embeddings are stored in an individual FAISS (Facebook AI Similarity Search) index. FAISS allows for highly efficient similarity searches.
        *   A **combined FAISS index** is created for the current session, aggregating embeddings from all documents you've uploaded in that session. This enables querying across your entire document set simultaneously.

4.  **Querying and Retrieval:**
    *   You type a question into the chat interface.
    *   **Query Embedding:** Your question is also converted into an embedding using the same Sentence Transformer model.
    *   **Similarity Search:** This query embedding is compared against the embeddings in the session's combined FAISS index. The system retrieves the top-K most similar text chunks from your documents.
    *   **Conversation History:** Your recent conversation history within the session is retrieved to provide context to the LLM.

5.  **Answer Generation (LLM Interaction):**
    *   **Contextual Prompting:** The retrieved text chunks (the "context") and your conversation history are combined with your original question to form a comprehensive prompt for a Large Language Model (LLM).
    *   **LLM Selection:** You can choose to use:
        *   **Local LLMs:** Models like DialoGPT, Zephyr, or Mistral running directly on your machine via the Hugging Face `transformers` library.
        *   **Cloud LLM APIs:** Services like OpenAI (GPT models), Google Gemini, or various models accessed through the AIML API (which can include Claude, other GPT versions, Llama models, etc.). API keys are managed via environment variables.
    *   **Response Generation:** The selected LLM processes the prompt and generates an answer based on the provided document context and conversation history.

6.  **Displaying Results:**
    *   The LLM's answer is streamed or sent back to the web interface.
    *   **Source Highlighting:** Crucially, the application also provides references to the specific text chunks (and their source documents) that were used by the LLM to formulate the answer, allowing you to verify the information.

This entire process allows for a dynamic and interactive way to extract information and insights from your PDF library.

The basic data flow can be visualized as:

```
[User via Next.js Frontend]
         |
         v
[Flask API Server (Python Backend)]
         |
         +---------------------------------+
         | 1. PDF Upload & Processing      |
         |    - Text Extraction (PyMuPDF)  |
         |    - Text Chunking              |
         |    - Store in SQLite            |
         +---------------------------------+
         | 2. Embedding & Indexing         |
         |    - Embed Chunks (Sentence Tx) |
         |    - Build FAISS Index (Session)|
         +---------------------------------+
         | 3. User Query                   |
         |    - Embed Query                |
         |    - FAISS Similarity Search    |
         |    - Retrieve Chunks & History  |
         +---------------------------------+
         | 4. LLM Answer Generation        |
         |    - Construct Prompt           |
         |    - Interact with chosen LLM   |
         |      (Local/Cloud)              |
         +---------------------------------+
         | 5. Display Response & Sources   |
         |
         v
[User via Next.js Frontend]
```

---

## Installation

Follow these steps to set up QuantumPDF ChatApp on your local machine.

### Prerequisites

-   **Python:** Version 3.8 or higher.
-   **Node.js:** Version 16 or higher (for the Next.js frontend).
-   **pip:** Python package installer (usually comes with Python).
-   **Git:** For cloning the repository.

### Setup Steps

1.  **Clone the Repository:**
    Open your terminal and run:
    ```bash
    git clone https://github.com/Kedhareswer/QuantumPDF_ChatApp.git
    cd QuantumPDF_ChatApp
    ```

2.  **Configure Environment Variables (API Keys):**
    This application may require API keys for cloud-based LLM services (like OpenAI, Google Gemini, AIML API).
    -   Create a file named `.env` in the root of the project directory (`QuantumPDF_ChatApp/.env`).
    -   Add your API keys to this file in the following format:
        ```env
        OPENAI_API_KEY=your_openai_api_key_here
        GEMINI_API_KEY=your_google_gemini_api_key_here
        AIML_API_KEY=your_aiml_api_key_here
        # Add other keys as needed, e.g., for Weights & Biases
        # WANDB_API_KEY=your_wandb_api_key_here
        ```
    *Note: The application will still run without these keys, but you'll be limited to using locally hosted models or features that don't require them.*

3.  **Set Up Python Backend:**
    In the project root directory (`QuantumPDF_ChatApp`):
    ```bash
    # Create a virtual environment (recommended)
    python -m venv venv
    # Activate the virtual environment
    # On macOS/Linux:
    source venv/bin/activate
    # On Windows:
    # .\venv\Scripts\activate

    # Install Python dependencies
    pip install -r requirements.txt
    ```

4.  **Set Up Next.js Frontend:**
    In the project root directory (`QuantumPDF_ChatApp`), open a **new terminal window/tab** and run:
    ```bash
    # Install Node.js dependencies
    npm install
    # (or if you use pnpm as suggested by pnpm-lock.yaml)
    # pnpm install
    ```
    *Note: The presence of `pnpm-lock.yaml` suggests `pnpm` might be the preferred package manager. If you have `pnpm` installed, using `pnpm install` is recommended.*

5.  **Run the Application:**
    *   **Start the Backend API:**
        In your first terminal (with the Python virtual environment activated):
        ```bash
        python app.py
        ```
        The backend will typically start on `http://localhost:5000`.

    *   **Start the Frontend Development Server:**
        In your second terminal (where you installed Node.js dependencies):
        ```bash
        npm run dev
        # (or if you use pnpm)
        # pnpm run dev
        ```
        The frontend will typically start on `http://localhost:3000`.

6.  **Access the Application:**
    Open your web browser and navigate to [http://localhost:3000](http://localhost:3000).

---

## Using QuantumPDF ChatApp

Once the application is running (both backend and frontend), you can start interacting with your PDFs:

1.  **Access the Web Interface:**
    *   Open your browser and go to `http://localhost:3000` (or the address shown when you started the frontend).

2.  **Upload Your PDFs:**
    *   Locate the PDF upload area on the page. This might be a button labeled "Upload PDF" or a drag-and-drop zone.
    *   Select one or more PDF documents you want to work with. The application will process them (extract text, chunk, embed).
    *   Uploaded documents in the current session will likely be listed in a "Document Library" or similar panel.

3.  **Configure Your Chat Session:**
    *   **Select a Language Model:** Find the model selection dropdown or options.
        *   **Local Models:** If you have set up local Hugging Face models and have sufficient hardware, you can choose one from the list (e.g., "DialoGPT-medium," "Zephyr-7b-beta"). These run on your computer.
        *   **Cloud Models:** To use models like OpenAI's GPT series, Google's Gemini, or others via AIML, select the appropriate option.
    *   **Enter API Key (if required):** If you choose a cloud-based model, an input field will likely appear for you to enter the corresponding API key (e.g., your OpenAI API key). Ensure you have configured these in your `.env` file or be prepared to enter them in the UI if the application supports it directly. *It's generally more secure and convenient to use the `.env` file method.*

4.  **Start Chatting:**
    *   Once your documents are processed and your model is selected, use the chat input field (usually at the bottom of the chat interface) to type your questions about the PDF content.
    *   Press Enter or click a "Send" button.

5.  **Review Responses and Sources:**
    *   The LLM's answer will appear in the chat window.
    *   Pay attention to any **source citations** provided with the answer. These indicate which parts of your uploaded PDFs were used to generate the response, allowing you to verify accuracy.

6.  **Manage Documents and Conversation:**
    *   **Multiple Documents:** If you've uploaded multiple PDFs, your questions will be addressed by searching across all of them within the current session.
    *   **Document List:** You can usually view the list of currently active documents and may have options to remove them from the session.
    *   **Conversation History:** The chat interface will display your ongoing conversation. Some applications offer an option to clear the conversation history and start fresh.

**Example Questions:**

*   "What is the main conclusion of the document [document_name.pdf]?"
*   "Summarize section 3 of [report.pdf]."
*   "Compare the methodologies described in [paper_A.pdf] and [paper_B.pdf]."
*   "What are the key recommendations regarding [topic]?"

Experiment with different questions and models to get the most out of QuantumPDF ChatApp!

---

## Configuration

QuantumPDF ChatApp offers several ways to configure its behavior:

1.  **Environment Variables (`.env` file):**
    *   As detailed in the "Installation" section, the primary way to provide API keys for cloud-based LLM services (OpenAI, Google Gemini, AIML API) and other integrations like Weights & Biases is through a `.env` file in the project root.
    *   Example content for `.env`:
        ```env
        OPENAI_API_KEY=your_openai_key
        GEMINI_API_KEY=your_gemini_key
        AIML_API_KEY=your_aiml_key
        WANDB_API_KEY=your_wandb_key
        ```

2.  **LLM Selection (via UI and Backend):**
    *   **Frontend Interface:** You can typically select your desired LLM (local or cloud-based) from the options presented in the web UI.
    *   **Backend (`app.py`):** The list of available local Hugging Face models and specific cloud models (e.g., via AIML API) is defined in `app.py` (see the `/models` endpoint logic and `MultiDocumentRAG` class). To add or change default local models, you would modify this file.

3.  **Text Processing Parameters (Backend Code):**
    *   **Embedding Model:** The default sentence transformer model for creating text embeddings is `sentence-transformers/all-MiniLM-L6-v2`. This is specified in the `MultiDocumentRAG` class in `app.py`. Changing this requires modifying the `load_embedding_model` method.
    *   **Text Chunking:** The `chunk_text` method in `app.py` uses default parameters:
        *   `chunk_size`: Approximately 500 characters.
        *   `overlap`: Approximately 50 characters between chunks.
        These values can be adjusted in the code for different chunking strategies.

4.  **Conversation Memory (Backend Code):**
    *   The `ConversationMemory` class in `app.py` has a `max_history` parameter, defaulting to 10 conversation turns (user message + assistant response). This controls how much of the recent conversation is kept as context for the LLM. Modifying this requires changing the class instantiation.

5.  **Data Storage:**
    *   **Vector Embeddings (FAISS):** FAISS is used for in-memory storage and searching of text embeddings. While highly efficient for session-based activity, these indexes are typically rebuilt when the application restarts or sessions change.
    *   **Document & Chunk Persistence (SQLite):** The `DocumentManager` class uses an SQLite database (`documents.db`) to store metadata about uploaded documents and their extracted text chunks. This allows for some persistence of processed data across application runs, associated with session IDs.

6.  **Extending to Other Databases:**
    *   The current setup uses FAISS for vector search and SQLite for metadata. The "Extending" section notes that the architecture is open to integrating other vector databases (e.g., Pinecone, Weaviate) with code modifications.

---

## Performance

- **Recommended RAM:** 8GB+; 16GB+ for local LLMs.
- **GPU:** Use CUDA-enabled PyTorch for faster performance.
- **Tips:** Use larger embedding models for accuracy, enable caching for speed.

---

## Troubleshooting

- **PDF Errors:** Ensure the PDF has extractable text and is not excessively large.
- **Model Issues:** Check RAM and internet connectivity.
- **API Issues:** Verify API keys and check for rate limits.
- **Logs:** Review application output for error details.

---

## Extending

- To add models: Update `app.py` and frontend model selection.
- To use custom embeddings: Replace the default embedding model.
- To use persistent vector databases: Replace FAISS as needed.

---

## License

GNU General Public License v3.0. See [LICENSE](LICENSE) for details.

---

## Community & Support

- [GitHub Discussions](https://github.com/Kedhareswer/QuantumPDF_ChatApp/discussions)
- [Issues](https://github.com/Kedhareswer/QuantumPDF_ChatApp/issues)
---
